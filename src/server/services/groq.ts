

import Groq from 'groq-sdk';
import { ReadableStream } from 'stream/web'; // Node.js í™˜ê²½ì—ì„œ ìŠ¤íŠ¸ë¦¼ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•„ìš”

// ğŸš¨ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë°©ì‹ ìˆ˜ì •: process.env ëŒ€ì‹  ì§ì ‘ ì ‘ê·¼
// Next.js í™˜ê²½ì—ì„œëŠ” process.envê°€ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë¯€ë¡œ, 
// Groq í´ë¼ì´ì–¸íŠ¸ê°€ .envì˜ GROQ_API_KEYë¥¼ ì§ì ‘ ì½ë„ë¡ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
const groq = new Groq({
    apiKey: process.env.GROQ_API_KEY, 
});

// --- 1. ì„ë² ë”© ëª¨ë¸ ì •ì˜ ë° Mock ë¡œì§ ---

// GroqëŠ” ìì²´ ì„ë² ë”© APIë¥¼ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, RAG í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ Mock ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
// ì´ ë²¡í„°ì˜ ì°¨ì›(768)ì€ ì¼ë°˜ì ì¸ ì„ë² ë”© ëª¨ë¸(ì˜ˆ: bge-small, text-embedding-3-small)ì— ë§ì¶° ì¡°ì •í–ˆìŠµë‹ˆë‹¤.
const MOCK_VECTOR_DIMENSION = 768; 

/**
 * ğŸ¤– [ì„ì‹œ êµ¬í˜„] ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
 * * @param text - ì„ë² ë”©í•  í…ìŠ¤íŠ¸
 * @returns Mock ë²¡í„° ë°°ì—´ (number[])
 */
export async function createEmbedding(text: string): Promise<number[]> {
    // âš ï¸ ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì—¬ê¸°ì— OpenAIë‚˜ Cohere ë“±ì˜ ì„ë² ë”© ì „ìš© API í˜¸ì¶œ ë¡œì§ì´ ë“¤ì–´ê°€ì•¼ í•©ë‹ˆë‹¤.
    if (!text || text.length === 0) {
        return [];
    }
    
    // API í˜¸ì¶œ ì—†ì´ ì„ì‹œ(Mock) ë²¡í„° ë°˜í™˜
    console.warn("âš ï¸ MOCK EMBEDDING: Groq SDKê°€ ì„ë² ë”©ì„ ì§€ì›í•˜ì§€ ì•Šì•„ Mock ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.");
    return Array.from({ length: MOCK_VECTOR_DIMENSION }, () => Math.random());
}


// --- 2. RAG ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ---

const GENERATION_MODEL = 'mixtral-8x7b-32768'; // ë¹ ë¥´ê³  ê°•ë ¥í•œ Groq ëª¨ë¸ ì¶”ì²œ

/**
 * ğŸ’¬ RAGì™€ ê²°í•©ëœ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± (rag.tsì—ì„œ í˜¸ì¶œ)
 * @param systemPrompt - ì±—ë´‡ì˜ ê¸°ë³¸ ì—­í•  ì§€ì¹¨
 * @param question - ì‚¬ìš©ì ì§ˆë¬¸
 * @param context - RAGë¡œ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸
 * @returns ReadableStream ê°ì²´
 */
export async function generateStreamingResponse(
    systemPrompt: string,
    question: string,
    context?: string
): Promise<ReadableStream> {
    
    try {
        // RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        const finalSystemPrompt = context
            ? `${systemPrompt}\n\në‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•œ ì°¸ê³  ì •ë³´ì…ë‹ˆë‹¤:\n\n${context}\n\nìœ„ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.`
            : systemPrompt;

        const stream = await groq.chat.completions.create({
            model: GENERATION_MODEL,
            messages: [
                { role: 'system', content: finalSystemPrompt },
                { role: 'user', content: question },
            ],
            temperature: 0.2, 
            stream: true, 
        });
        
        // Groq SDK ìŠ¤íŠ¸ë¦¼ì„ Next.jsì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ReadableStreamìœ¼ë¡œ ë³€í™˜
        const encoder = new TextEncoder();
        
        return new ReadableStream({
            async start(controller) {
                for await (const chunk of stream) {
                    const content = chunk.choices[0]?.delta?.content || '';
                    if (content) {
                        controller.enqueue(encoder.encode(content));
                    }
                }
                controller.close();
            },
        }) as ReadableStream;

    } catch (error) {
        console.error('Groq Streaming Error:', error);
        throw new Error('Groq API ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (API í‚¤, ë„¤íŠ¸ì›Œí¬ í™•ì¸)');
    }
}